#1 a)
library(tidyverse)
# load both datasets one is the first five keywords (Python, Java, Typescript, Interest Rates, Regulation) worldwide
#Second one is (Volatility, NHL, Ice Skates, Sports Training) #here ice hockey is seasonal
df1 <- read_csv("multiTimeline.csv", skip = 2) #first two rows are just text from google trends and empty column.
df1 <- df1 %>% rename(date = 1)
df2 <- df2 %>% rename(date = 1)
# convert to date
df1$date <- as.Date(df1$date)
df2$date <- as.Date(df2$date)
# full join to combine all dates
df <- full_join(df1, df2, by = "date") %>%
arrange(date)
# full join to combine all dates
df <- full_join(df1, df2, by = "date") %>%
arrange(date)
```{r}
df_clean <- df %>% #a lot of wired values in the google dataset
mutate(across(
.cols = -date,
.fns = ~ {
x <- str_trim(.x)
x <- str_replace_all(x, "<1", "0")
x <- str_replace_all(x, "--", "0")
x <- as.numeric(x)
x[is.na(x)] <- 0   # remove NA
x
}
))
names(df_clean) <- names(df_clean) |> #remove the "(Worldwide)" after each keyword
str_replace(": \\(Worldwide\\)", "") |>
str_trim()
```{r, Exercise 1 b), fig.width=10, fig.height=5}
df_long <- df_clean %>%
pivot_longer(
cols = -date,
names_to = "keyword",
values_to = "value"
)
ggplot(df_long, aes(date, value)) +
geom_line(color = "steelblue") +
facet_wrap(~ keyword, scales = "free_y") +
theme_minimal(base_size = 14) +
labs(
title = "Google Trends by Keyword",
x = "Date",
y = "Interest"
)
ggplot(df_long, aes(keyword, value, fill = keyword)) +
geom_violin(alpha = 0.6) +
geom_boxplot(width = 0.15, color = "black", alpha = 0.8) +
theme_minimal(base_size = 14) +
labs(
title = "Distribution of Google Trends Values",
x = "",
y = "Search Interest"
) +
theme(legend.position = "none")
library(forecast)
library(gridExtra)
# 3 years of weekly data
lags <- 52 * 3
#ACF plots
acf_plots <- df_long %>%
group_by(keyword) %>%
summarise(ts = list(ts(value, frequency = 52))) %>%
mutate(plot = map2(ts, keyword, ~{
ggAcf(.x, lag.max = lags) +
ggtitle(paste("ACF -", .y)) +
theme_minimal()
}))
do.call("grid.arrange", c(acf_plots$plot, ncol = 3))
pacf_plots <- df_long %>%
group_by(keyword) %>%
summarise(ts = list(ts(value, frequency = 52))) %>%
mutate(plot = map2(ts, keyword, ~{
ggPacf(.x, lag.max = lags) +
ggtitle(paste("PACF -", .y)) +
theme_minimal()
}))
do.call("grid.arrange", c(pacf_plots$plot, ncol = 3))
library(dtw)
library(pheatmap)
ts_mat <- df_clean %>%
select(-date) %>%
as.matrix()
ts_mat <- df_clean %>% select(-date) %>% as.matrix() #simple point-wise difference, ignores temporal shifts.
dist_euclid <- dist(t(ts_mat), method = "euclidean")
dist_corr <- as.dist(1 - cor(ts_mat)) #distance correlation. Captures similarity in shape regardless of scale.
keywords <- colnames(ts_mat)
dist_dtw <- matrix(0, ncol(ts_mat), ncol(ts_mat))
rownames(dist_dtw) <- colnames(dist_dtw) <- keywords
for(i in 1:ncol(ts_mat)){
for(j in 1:ncol(ts_mat)){
dist_dtw[i,j] <- dtw(ts_mat[,i], ts_mat[,j])$distance
}
}
dist_dtw <- as.dist(dist_dtw) #dynamic time warping distance
dist_arima <- matrix(0, ncol(ts_mat), ncol(ts_mat)) #ARIMA-based dissimilarity
rownames(dist_arima) <- colnames(dist_arima) <- colnames(ts_mat)
for(i in 1:ncol(ts_mat)){
for(j in 1:ncol(ts_mat)){
fit1 <- auto.arima(ts_mat[,i])
fit2 <- auto.arima(ts_mat[,j])
dist_arima[i,j] <- abs(fit1$aic - fit2$aic)
}
}
for(i in 1:ncol(ts_mat)){
for(j in 1:ncol(ts_mat)){
fit1 <- auto.arima(ts_mat[,i])
fit2 <- auto.arima(ts_mat[,j])
dist_arima[i,j] <- abs(fit1$aic - fit2$aic)
}
}
dist_arima <- as.dist(dist_arima)
```{r}
```{r}
library(dtw)
library(pheatmap)
ts_mat <- df_clean %>%
select(-date) %>%
as.matrix()
ts_mat <- df_clean %>% select(-date) %>% as.matrix() #simple point-wise difference, ignores temporal shifts.
dist_euclid <- dist(t(ts_mat), method = "euclidean")
dist_corr <- as.dist(1 - cor(ts_mat)) #distance correlation. Captures similarity in shape regardless of scale.
keywords <- colnames(ts_mat)
dist_dtw <- matrix(0, ncol(ts_mat), ncol(ts_mat))
rownames(dist_dtw) <- colnames(dist_dtw) <- keywords
for(i in 1:ncol(ts_mat)){
for(j in 1:ncol(ts_mat)){
dist_dtw[i,j] <- dtw(ts_mat[,i], ts_mat[,j])$distance
}
}
for(i in 1:ncol(ts_mat)){
for(j in 1:ncol(ts_mat)){
dist_dtw[i,j] <- dtw(ts_mat[,i], ts_mat[,j])$distance
}
}
dist_dtw <- as.dist(dist_dtw) #dynamic time warping distance
dist_arima <- matrix(0, ncol(ts_mat), ncol(ts_mat)) #ARIMA-based dissimilarity
rownames(dist_arima) <- colnames(dist_arima) <- colnames(ts_mat)
for(i in 1:ncol(ts_mat)){
for(j in 1:ncol(ts_mat)){
fit1 <- auto.arima(ts_mat[,i])
fit2 <- auto.arima(ts_mat[,j])
dist_arima[i,j] <- abs(fit1$aic - fit2$aic)
}
}
dist_arima <- as.dist(dist_arima)
```{r}
pheatmap(as.matrix(dist_euclid), main = "Euclidean Distance")
pheatmap(as.matrix(dist_corr), main = "Correlation Distance")
pheatmap(as.matrix(dist_dtw), main = "DTW Distance")
pheatmap(as.matrix(dist_arima), main = "ARIMA Model Distance")
library(ggplot2)
library(ggdendro)
library(gridExtra)
plot_dend <- function(dist_matrix, title){
hc <- hclust(dist_matrix, method = "complete")
ggdendrogram(hc, rotate = TRUE, size = 2) +
ggtitle(title) +
theme_minimal()
}
p1 <- plot_dend(dist_euclid, "Euclidean Distance")
p2 <- plot_dend(dist_corr, "Correlation Distance")
p3 <- plot_dend(dist_dtw, "DTW Distance")
p4 <- plot_dend(dist_arima, "ARIMA Model Distance")
grid.arrange(p1, p2, p3, p4, ncol = 2)
library(mclust)
# Perform hierarchical clustering
hc_euclid <- hclust(dist_euclid, method = "complete")
hc_corr   <- hclust(dist_corr,   method = "complete")
hc_dtw    <- hclust(dist_dtw,    method = "complete")
hc_arima  <- hclust(dist_arima,  method = "complete")
# Cut into 3 clusters (sports, programming, finance)
pred_euclid <- cutree(hc_euclid, k = 3)
pred_corr   <- cutree(hc_corr,   k = 3)
pred_dtw    <- cutree(hc_dtw,    k = 3)
pred_arima  <- cutree(hc_arima,  k = 3)
# True labels (make sure order matches your columns)
true_labels <- c(
"sports", "sports", "sports",       # Ice hockey, Ice skate, NHL
"programming", "programming", "programming",   # Python, Java, TypeScript
"finance", "finance", "finance"     # Interest rate, Regulation, Volatility
)
# Compute ARI
ari_euclid <- adjustedRandIndex(pred_euclid, true_labels)
ari_corr   <- adjustedRandIndex(pred_corr,   true_labels)
ari_dtw    <- adjustedRandIndex(pred_dtw,    true_labels)
ari_arima  <- adjustedRandIndex(pred_arima,  true_labels)
ari_euclid; ari_corr; ari_dtw; ari_arima
# correlation distance
dist_corr <- as.dist(1 - cor(ts_mat))
# hierarchical clustering
hc_corr <- hclust(dist_corr, method = "complete")
# cut into 3 clusters
cluster_assignments <- cutree(hc_corr, k = 3)
cluster_assignments
cluster_df <- data.frame(
keyword = colnames(ts_mat),
cluster = factor(cluster_assignments)
)
df_plot <- df_long %>%
left_join(cluster_df, by = "keyword")
```{r, fig.width=14, fig.height=6}
ggplot(df_plot, aes(x = date, y = value, color = cluster)) +
geom_line() +
facet_wrap(~ keyword, scales = "free_y", ncol = 3) +
theme_minimal(base_size = 13) +
labs(
title = "Time Series Clustered Using Correlation Distance",
x = "",
y = "Search Interest"
)
```{r}
The stock market is not a Markov process because future price movements depend on far more than just the current price. Past trends, volatility regimes, market sentiment, macro announcements, liquidity shocks, and structural breaks all influence how prices evolve, and this information is not contained in the current price alone. Even if you know today’s price, you are missing critical context like momentum, volatility clustering, and order flow dynamics. Because predicting tomorrow requires more than just the present state, the stock market violates the Markov property.
library(HiddenMarkov)
library(tidyverse)
library(HiddenMarkov)
library(patchwork)
kw1 <- "National Hockey League"
kw2 <- "Interest rate"
kw3 <- "Python"
#fit + plot one hmm
fit_and_plot_hmm <- function(df_k) {
df_k <- df_k %>% arrange(date)
x <- df_k$value
# Build model
model <- dthmm(
x      = x,
Pi     = matrix(c(0.9,0.1,
0.1,0.9), nrow = 2),
delta  = c(0.5, 0.5),
distn  = "norm",
pm     = list(
mean = c(mean(x) - sd(x), mean(x) + sd(x)),
sd   = c(sd(x), sd(x))
)
)
# Fit with Baum–Welch
fit <- BaumWelch(model)
# Decode most likely states
states <- Viterbi(fit)
df_k$state <- factor(states)
# Plot
ggplot(df_k, aes(date, value, color = state)) +
geom_line(size = 1) +
theme_minimal() +
labs(
title = paste("HMM (2 states) for", df_k$keyword[1]),
x = "Date",
y = "Value",
color = "State"
)
}
#fit and plot
df_NHL <- df_long %>% filter(keyword == kw1)
p1 <- fit_and_plot_hmm(df_NHL)
p2 <- fit_and_plot_hmm(df_IR)
library(tidyverse)
library(HiddenMarkov)
library(HiddenMarkov)
library(patchwork)
kw1 <- "National Hockey League"
kw2 <- "Interest rate"
kw3 <- "Python"
#fit + plot one hmm
fit_and_plot_hmm <- function(df_k) {
df_k <- df_k %>% arrange(date)
x <- df_k$value
# Build model
model <- dthmm(
x      = x,
Pi     = matrix(c(0.9,0.1,
0.1,0.9), nrow = 2),
delta  = c(0.5, 0.5),
distn  = "norm",
pm     = list(
mean = c(mean(x) - sd(x), mean(x) + sd(x)),
sd   = c(sd(x), sd(x))
)
)
# Fit with Baum–Welch
fit <- BaumWelch(model)
# Decode most likely states
states <- Viterbi(fit)
df_k$state <- factor(states)
# Plot
ggplot(df_k, aes(date, value, color = state)) +
geom_line(size = 1) +
theme_minimal() +
labs(
title = paste("HMM (2 states) for", df_k$keyword[1]),
x = "Date",
y = "Value",
color = "State"
)
}
#fit and plot
df_NHL <- df_long %>% filter(keyword == kw1)
p1 <- fit_and_plot_hmm(df_NHL)
df_IR <- df_long %>% filter(keyword == kw2)
p2 <- fit_and_plot_hmm(df_IR)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
# Show plots
combined <- p1 / p2 / p3
library(HiddenMarkov)
library(tidyverse)
library(HiddenMarkov)
library(patchwork)
kw1 <- "National Hockey League"
kw2 <- "Interest rate"
kw3 <- "Python"
#fit + plot one hmm
fit_and_plot_hmm <- function(df_k) {
df_k <- df_k %>% arrange(date)
x <- df_k$value
# Build model
model <- dthmm(
x      = x,
Pi     = matrix(c(0.9,0.1,
0.1,0.9), nrow = 2),
delta  = c(0.5, 0.5),
distn  = "norm",
pm     = list(
mean = c(mean(x) - sd(x), mean(x) + sd(x)),
sd   = c(sd(x), sd(x))
)
)
# Fit with Baum–Welch
fit <- BaumWelch(model)
# Decode most likely states
states <- Viterbi(fit)
df_k$state <- factor(states)
# Plot
ggplot(df_k, aes(date, value, color = state)) +
geom_line(size = 1) +
theme_minimal() +
labs(
title = paste("HMM (2 states) for", df_k$keyword[1]),
x = "Date",
y = "Value",
color = "State"
)
}
#fit and plot
df_NHL <- df_long %>% filter(keyword == kw1)
p1 <- fit_and_plot_hmm(df_NHL)
df_IR <- df_long %>% filter(keyword == kw2)
p2 <- fit_and_plot_hmm(df_IR)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
# Show plots
combined <- p1 / p2 / p3
combined
p1 <- fit_and_plot_hmm(df_NHL)
p2 <- fit_and_plot_hmm(df_IR)
#fit and plot
df_NHL <- df_long %>% filter(keyword == kw1)
p1 <- fit_and_plot_hmm(df_NHL)
df_IR <- df_long %>% filter(keyword == kw2)
p2 <- fit_and_plot_hmm(df_IR)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
library(HiddenMarkov)
library(tidyverse)
library(HiddenMarkov)
library(patchwork)
kw1 <- "National Hockey League"
kw2 <- "Interest rate"
kw3 <- "Python"
#fit + plot one hmm
fit_and_plot_hmm <- function(df_k) {
df_k <- df_k %>% arrange(date)
x <- df_k$value
# Build model
model <- dthmm(
x      = x,
Pi     = matrix(c(0.9,0.1,
0.1,0.9), nrow = 2),
delta  = c(0.5, 0.5),
distn  = "norm",
pm     = list(
mean = c(mean(x) - sd(x), mean(x) + sd(x)),
sd   = c(sd(x), sd(x))
)
)
# Fit with Baum–Welch
fit <- BaumWelch(model, silent = TRUE)
# Decode most likely states
states <- Viterbi(fit)
df_k$state <- factor(states)
# Plot
ggplot(df_k, aes(date, value, color = state)) +
geom_line(size = 1) +
theme_minimal() +
labs(
title = paste("HMM (2 states) for", df_k$keyword[1]),
x = "Date",
y = "Value",
color = "State"
)
}
df_IR <- df_long %>% filter(keyword == kw2)
p2 <- fit_and_plot_hmm(df_IR)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
library(HiddenMarkov)
library(tidyverse)
library(HiddenMarkov)
library(patchwork)
kw1 <- "National Hockey League"
kw2 <- "Interest rate"
kw3 <- "Python"
#fit + plot one hmm
fit_and_plot_hmm <- function(df_k) {
df_k <- df_k %>% arrange(date)
x <- df_k$value
# Build model
model <- dthmm(
x      = x,
Pi     = matrix(c(0.9,0.1,
0.1,0.9), nrow = 2),
delta  = c(0.5, 0.5),
distn  = "norm",
pm     = list(
mean = c(mean(x) - sd(x), mean(x) + sd(x)),
sd   = c(sd(x), sd(x))
)
)
# Fit with Baum–Welch
fit <- suppressMessages( #remove the unnecessary printing to console for pdf
suppressWarnings(#readability
BaumWelch(model)
)
)
# Decode most likely states
states <- Viterbi(fit)
df_k$state <- factor(states)
# Plot
ggplot(df_k, aes(date, value, color = state)) +
geom_line(size = 1) +
theme_minimal() +
labs(
title = paste("HMM (2 states) for", df_k$keyword[1]),
x = "Date",
y = "Value",
color = "State"
)
}
df_IR <- df_long %>% filter(keyword == kw2)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
#fit and plot
df_NHL <- df_long %>% filter(keyword == kw1)
p1 <- fit_and_plot_hmm(df_NHL)
df_IR <- df_long %>% filter(keyword == kw2)
p2 <- fit_and_plot_hmm(df_IR)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
# Show plots
combined <- p1 / p2 / p3
library(HiddenMarkov)
library(tidyverse)
library(HiddenMarkov)
library(patchwork)
kw1 <- "National Hockey League"
kw2 <- "Interest rate"
kw3 <- "Python"
#fit + plot one hmm
fit_and_plot_hmm <- function(df_k) {
df_k <- df_k %>% arrange(date)
x <- df_k$value
# Build model
model <- dthmm(
x      = x,
Pi     = matrix(c(0.9,0.1,
0.1,0.9), nrow = 2),
delta  = c(0.5, 0.5),
distn  = "norm",
pm     = list(
mean = c(mean(x) - sd(x), mean(x) + sd(x)),
sd   = c(sd(x), sd(x))
)
)
# Fit with Baum–Welch
tmp <- capture.output(
fit <- BaumWelch(model)
)
# Decode most likely states
states <- Viterbi(fit)
df_k$state <- factor(states)
# Plot
ggplot(df_k, aes(date, value, color = state)) +
geom_line(size = 1) +
theme_minimal() +
labs(
title = paste("HMM (2 states) for", df_k$keyword[1]),
x = "Date",
y = "Value",
color = "State"
)
}
p1 <- fit_and_plot_hmm(df_NHL)
df_IR <- df_long %>% filter(keyword == kw2)
p2 <- fit_and_plot_hmm(df_IR)
df_Python <- df_long %>% filter(keyword == kw3)
p3 <- fit_and_plot_hmm(df_Python)
# Show plots
combined <- p1 / p2 / p3
combined
